---
title: "Tarea 6: regresión logística"
format: html
---

## Regresión logística y regularización

Considera los siguientes datos de entrenamiento donde queremos predecir la 
categoria y en función de x usando regresión logística, es decir, la probabilidad
de que la $y$ sea igual a 1 es igual a: 

$$p(x) = h(a + bx)$$

donde $a$ y $b$ son parámetros que queremos ajustar y $h$ es la función sigmoide
que vimos en clase.


```{r}
#| message: false
library(tidyverse)
datos_ent_tbl <- tibble(
  x = c(-2, -2.5, -1, 1.5, 1.6, 2.7, 3, 3.5), 
  y = c(0, 0, 0, 1, 0, 1, 1, 1)) |>
  mutate(x = x - mean(x)) # centrar x
ggplot(datos_ent_tbl, aes(x = x, y = y, colour = factor(y) )) +
  geom_point()
```

Usaremos estas funciones auxiliares:

```{r}
h <- function(x){
  1/(1 + exp(-x))
  # o exp(x) / (1 + exp(x)), que es lo mismo
}
perdida_log <- function(p, y){
  -mean(y * log(p) + (1-y)*log(1-p))
}
evaluar_h <- function(x, params){
  a = params[1]
  b = params[2]
  prob <- h(a + b * x)
  tibble(x = x, p = prob)
}
calc_perdida_log <- function(datos){
  perdida_fun <- function(params){
    resultado <- perdida_log(
      p = evaluar_h(x = datos$x, params) |> pull(p),
      y = datos$y)
    resultado
  }
  perdida_fun
}
```

En el siguiente código, podemos proponer coeficientes a y b de regresión logística
y ver cómo ajusta el modelo

```{r}
a <- 0
b <- -0.2
perdida_log_ent <- calc_perdida_log(datos_ent_tbl)
modelo_logistico <- evaluar_h(x = seq(-3, 3, 0.01), params = c(a, b))
ggplot(datos_ent_tbl, aes(x = x)) +
  geom_point(aes(y = y, colour = factor(y))) +
  geom_line(data = modelo_logistico, aes(y = p)) +
  labs(subtitle = sprintf("Pérdida logarítmica: %.3f", perdida_log_ent(c(a,b))))
```

*Pregunta 1*: Cambiando los valores de a y b busca minimizar la pérdida logarítmica
aproximadamente. ¿Tu solución da probabilidades muy cercanas a 0 y 1 para todos los
valores de $x$?

Sugerencia: 1) Cambia uno de los parámetros por una cantidad
**no muy grande**. Checa si la pérdida sube o baja. 2) Si baja, entonces regresa a 1 con
el otro parámetro. 3) Si no baja, mueve el parámetro en dirección contraria, hasta
que la pérdida baje, 4) Repite 1 alternando parámetros.


**Pregunta 2**: usando el siguiente código o algo similares, encuentra los valores óptimos.
¿Qué optimizador se usa en este caso (consulta la documentación)? ¿Qué tan cercanos
son a los valores que encontraste "manualmente" en la pregunta anterior?
Si usaste otra función
contesta con el optimizador que utiliza esa función.

```{r}
glm(y ~ x, datos_ent_tbl, family = "binomial")
```

Verifica que un optimizador genérico, para este problema, da la misma solución
que la que vimos arriba, minimzando la pérdida logarítmica:

```{r}
optim(par = c(0, 0), fn = perdida_log_ent)
```

## Regularización

Si queremos hacer regularización, entonces es necesario cambiar la
función objetivo que queremos minimizar:

```{r}
calc_perdida_log_reg <- function(datos, lambda = 0){
  perdida_fun <- function(params){
    resultado <- perdida_log(
      p = evaluar_h(x = datos$x, params) |> pull(p),
      y = datos$y)
    # sumar regularización a la pérdia
    resultado + lambda * params[2] ^2
  }
  perdida_fun
}
```

Seleccionamos un valor de lambda y podemos minimizar:

```{r}
perdida_log_reg_ent <- calc_perdida_log_reg(datos_ent_tbl, lambda = 0.000001)
optim(par = c(0, 0), fn = perdida_log_reg_ent)
```

**Pregunta 4**: ¿por qué en el ejemplo anterior obtienes una solución muy
similar al de la pregunta anterior (mismos coeficientes)? Incrementa $\lambda$ y describe
qué sucede con los coeficientes. 


**Pregunta 5** Explica por qué una solución regularizada podría desempeñarse mejor
que el modelo que ajustamos sin regularización. Por ejemplo, explica por qué con 
pocos datos es riesgoso que algunas regiones tengan probabilidades muy cercanas a 
0 o 1.


## Regresión logística multinomial


Consideramos un problema de regresión logística multinomial con tres clases, y una
sola variable de entrada. Por ejemplo:

```{r}
datos_tbl <- tibble(
  id = 1:10, 
  x = c(-2, -2, -1, 0, 1, 1,5, 2, 3, 3.5),
  clase = c("a", "a", "b", "a", "b", "b", "c", "b", "c", "c"))
datos_tbl
```


Construiremos paso por paso un modelo logístico multinomial. En este caso,
escogeremos los parámetros de manera manual, pero más adelante veremos cómo
ajustarlos con los datos.

Ponemos para cada clase un predictor lineal:

$f_a(x) = -0.5  - 0.1 x$, $f_b(x) =  0.3  x$, $f_c(x) = 0.6  x$

Que calculamos como sigue:

```{r}
probas <- datos_tbl |> 
  tibble(a = -0.5 - 0.1 * x, b =  0.3 * x, c = 0.6 * x)
probas
```

Pero en a, b y c no tenemos probabilidades (por ejemplo hay valores negativos). 
Según el modelo multinomial, tenemos que obtener el *softmax* de estas cantidades.
Primero tomamos exponencial:

```{r}
probas <- probas |>
  mutate(across(c("a", "b", "c"), ~ exp(.x)))
probas
```

Y ahora normalizamos para que a, b y c sumen 1:

```{r}
probas <- probas |>
  mutate(suma = a + b + c) |> 
  mutate(across(c("a", "b", "c"), ~ .x / suma))
probas
```
Y vemos que en cada renglón a, b, y c suman 1. Estas son las probabilidades
de nuestro modelo logístico multinomial. Podemos graficarlas:


```{r}
probas |> select(x, a, b, c) |> pivot_longer(c("a", "b", "c"), names_to = "clase", values_to = "prob") |> 
  ggplot(aes(x = x, y = prob, colour = clase)) +
  geom_point() + geom_line()
```

**Pregunta 6**: ¿Las probabilidades de cada clase en un modelo logístico multinomial
tienen que ser siempre crecientes o decrecientes? Explica por qué no pasa esto, aún cuando 
las funciones $f_a, f_b, f_c$ son lineales.


**Pregunta 7**: escribe un modelo que de las mismas probabilidades, pero que
tenga $f_c(x) = 0$ para toda $x$ ¿Eso quiere decir que la probabilidad de $c$ no cambia para 
ninguna $x$

Sugerencia: empieza calculando:

```{r}
probas <- datos_tbl |> 
  tibble(a = -0.5 - 0.1 * x - 0.6 * x, b =  0.3 * x - 0.6*x, c = 0)
probas
```

y luego calcula con el código de arriba los probabilidades. Verifica que son iguales.

**Pregunta 8**: Según esto, explica por qué en un modelo de regresión logística multinomial
podemos escoger una clase de referencia para la que hacemos su función $f$ correspodiente
igual a 0 (coeficientes igual a 0), y esto no implica ninguna restricción adicional.
Como casi siempre usamos regularización, generalmente no es necesario hacer este paso.

Ahora ajustamos un modelo de regresión logística regularizada a estos datos:

```{r}
library(keras)
modelo_mult <- keras_model_sequential()
# modelo para tres clases:
modelo_mult |> 
  layer_dense(units = 3,
              activity_regularizer = regularizer_l2(l = 0.0001), 
              activation = 'softmax') 
# pérdida logarítmica multinomial = entropía cruzada categórica
modelo_mult |>  compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_sgd(learning_rate = 0.2)
)
x_entrena <- matrix(datos_tbl$x, ncol = 1)
# para keras, numerar las clases
y_num <- matrix(as.numeric(factor(datos_tbl$clase)) - 1, ncol = 1)
y_entrena <- to_categorical(y_num, num_classes = 3)
# ajustar
modelo_mult |> fit(
  x_entrena, y_entrena,
  batch_size = 10,
  epochs = 50, verbose = 1
  )
```


Grafica el  modelo. Primero obtenemos las probabilidades

```{r}
probas_mod <- predict(modelo_mult, x = x_entrena)
probas_mod
```

```{r}
colnames(probas_mod) <- c("a", "b", "c")
probas_mod_tbl <- probas_mod |> as_tibble() |> 
  mutate(x = datos_tbl$x) |> 
  pivot_longer(c("a", "b", "c"), names_to = "clase", values_to = "prob")

```



```{r}
probas_mod_tbl |> 
  ggplot(aes(x = x, y = prob, colour = clase)) +
  geom_point() + geom_line() + ylim(c(0,1))
```

**Pregunta 9**: ¿qué pasa con estas probabildades si pones un valor de regularización
considerablemente más alto? ¿Por qué este modelo que considera tres predictores lineales
separados puede ser que no funcione bien sin regularización?

Checa los coeficientes de los predictores lineales aquí:

```{r}
get_weights(modelo_mult)
```


